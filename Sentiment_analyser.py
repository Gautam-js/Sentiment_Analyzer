# -*- coding: utf-8 -*-
"""Movie_Review.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kBnj0CUjKxGoQNF8Cgvt_VkZgi7oMkjm
"""

# from google.colab import drive
# drive.mount('/content/drive')

import pandas as pd

#Specify the file path of the uploaded file:
file_path_train = 'C:\\Users\\gautam jyothish\\Desktop\\Project\\imdb_train.csv'  # Replace 'your_uploaded_file.csv' with the actual file name

#Read the file into a pandas DataFrame:
df = pd.read_csv(file_path_train)

#Specify the file path of the uploaded file:
file_path_test = 'C:\\Users\\gautam jyothish\\Desktop\\Project\\imdb_test.csv'  # Replace 'your_uploaded_file.csv' with the actual file name

#Read the file into a pandas DataFrame:
test_df = pd.read_csv(file_path_test)

#Lower case the Text column in training dataset:
df['text'] = df['text'].str.lower()
test_df['text'] = test_df['text'].str.lower()

#Save the DataFrame back to CSV:
df.to_csv("imdb_train_lowercased.csv", index=False)
test_df.to_csv("imdb_test_lowercased.csv", index= False)

print(df['text'])
print(test_df['text'])

import re

#Define a function to remove HTML tags using regular expressions:
def remove_html_tags(text):
  clean_text  = re.sub(r'<[^>]*>',  '', text)
  return clean_text

#Apply the function to the 'Text' column:
df['text']  = df['text'].apply(remove_html_tags)
test_df['text'] = test_df['text'].apply(remove_html_tags)

#Save the DataFrame back to CSV:
df.to_csv("imdb_no_html.csv", index=False)
test_df.to_csv("imdb_test_no_html.csv", index=False)

print("df_without_html:", df['text'])
print("test_df_without_html:", test_df['text'])

#Define a function to remove punctuations using regular expressions:
def remove_punctuation(text):
    clean_text = re.sub(r'[^\w\s]', '', text)
    return clean_text

#Apply the function to the 'review' column:
df['text'] = df['text'].apply(remove_punctuation)
test_df['text'] = test_df['text'].apply(remove_punctuation)

#Save the DataFrame back to CSV:
df.to_csv("imdb_train_no_punctuation.csv", index=False)
test_df.to_csv("imdb_test_no_punctuation.csv", index=False)

print(df['text'])
print(test_df['text'])

#Define a function to remove email IDs using regular expressions:
def remove_email(text):
    clean_text = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '', text)
    return clean_text

#Apply the function to the 'Text' column:
df['text'] = df['text'].apply(remove_email)
test_df["text"] = test_df['text'].apply(remove_email)

#Save the DataFrame back to CSV:
df.to_csv("imdb_train_no_email.csv", index=False)
test_df.to_csv("imdb_test_no_email.csv", index=False)

print(df['text'])
print(test_df['text'])

#Define a function to remove digits using regular expressions:
def remove_digits(text):
    clean_text = re.sub(r'\d+', '', text)
    return clean_text

#Apply the function to the 'Text' column:
df['text'] = df['text'].apply(remove_digits)
test_df['text'] = test_df['text'].apply(remove_digits)

#Save the DataFrame back to CSV:
df.to_csv("imdb_train_no_digits.csv", index=False)
test_df.to_csv("imdb_test_no_digits", index=False)

print(df['text'])
print(test_df['text'])

#Define a function to remove Unicode characters using regular expressions:
def remove_unicode(text):
    return re.sub(r'[^\x00-\x7F]+', '', str(text))

#Apply the function to the 'Text' column:
df['text'] = df['text'].apply(remove_unicode)
test_df['text'] = test_df['text'].apply(remove_unicode)

#Save the DataFrame back to CSV:
df.to_csv("imdb_train_no_unicode.csv", index=False)
test_df.to_csv("imdb_test_no_unicode.csv", index=False)

print(df['text'])
print(test_df['text'])

import nltk
from nltk.tokenize import word_tokenize

#Download NLTK resources:
nltk.download('punkt')

#Perform word tokenization on the 'Text' column:
df['tokens'] = df['text'].apply(word_tokenize)
test_df['tokens'] = test_df['text'].apply(word_tokenize)

print(df['tokens'])
print(test_df['tokens'])

from nltk.corpus import stopwords

#Download NLTK resource:
nltk.download('stopwords')

#Define list of stopwords:
stop_words = set(stopwords.words('english'))

#Define function to remove stopwords from a list of tokens:
def remove_stopwords(tokens):
  filtered_tokens = [word for word in tokens if word not in stop_words]
  return filtered_tokens

#Perform word tokenization on the 'tokens' column:
df['tokens'] = df['tokens'].apply(remove_stopwords)
test_df['tokens'] = test_df['tokens'].apply(remove_stopwords)

print(df['tokens'])
print(test_df['tokens'])

from nltk.stem import WordNetLemmatizer

#Download NLTK resources for lemmatization:
nltk.download('wordnet')

#Initialize lemmatizer:
lemmatizer = WordNetLemmatizer()

#Define function for lemmatization:
def lemmatize_tokens(tokens):
    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]
    return lemmatized_tokens

#Apply lemmatization to the 'tokens' column:
df['lemmatized_tokens'] = df['tokens'].apply(lemmatize_tokens)
test_df['lemmatized_tokens'] = test_df['tokens'].apply(lemmatize_tokens)

print(df['lemmatized_tokens'])
print(test_df['lemmatized_tokens'])

from nltk.tag import pos_tag

#Download NLTK resources for POS tagging:
nltk.download('averaged_perceptron_tagger')

#Define function for POS tagging:
def pos_tagging(tokens):
    pos_tags = pos_tag(tokens)
    return pos_tags

#Apply POS tagging to the 'lemmatized_tokens' column:
df['pos_tags'] = df['lemmatized_tokens'].apply(pos_tagging)
test_df['pos_tags'] = test_df['lemmatized_tokens'].apply(pos_tagging)

print(df['pos_tags'])
print(test_df['pos_tags'])

############################################################### Naive Bayes & Bag of words ###################################################################

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, roc_auc_score
import matplotlib.pyplot as plt

#Convert tuples into strings:
df['pos_tags_str'] = df['pos_tags'].apply(lambda tags: ' '.join([f"{word}_{tag}" for word, tag in tags]))
test_df['pos_tags_str'] = test_df['pos_tags'].apply(lambda tags: ' '.join([f"{word}_{tag}" for word, tag in tags]))

#Initialize CountVectorizer and fit it on training data:
vectorizer = CountVectorizer()
X_train = vectorizer.fit_transform(df['pos_tags_str'])
y_train = df['label']

#Train the Naive Bayes classifier:
nb_classifier = MultinomialNB()
nb_classifier.fit(X_train, y_train)

#Transform testing data using the same vectorizer:
X_test = vectorizer.transform(test_df['pos_tags_str'])
y_test = test_df['label']

#Predict on the testing data:
y_pred = nb_classifier.predict(X_test)

#Evaluate the model:

#Calculate accuracy:
accuracy = accuracy_score(y_test, y_pred)

#Calculate precision:
precision = precision_score(y_test, y_pred)

#Calculate recall:
recall = recall_score(y_test, y_pred)

#Calculate F1 score:
f1 = f1_score(y_test, y_pred)

#Print the evaluation metrics:
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)

#Calculate the predicted probabilities for the positive class:
y_pred_prob = nb_classifier.predict_proba(X_test)[:, 1]

#Calculate the ROC curve:
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)

#Calculate the Area Under the ROC Curve (AUC):
auc = roc_auc_score(y_test, y_pred_prob)

#Print AUC score
print("AUC Score:", auc)

#Plot ROC curve:

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label='ROC Curve (AUC = %0.2f)' % auc)
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

############################################################# KNeighborsClassifier & Bag of words ####################################################################
from sklearn.neighbors import KNeighborsClassifier

#Initialize CountVectorizer and fit it on training data:
vectorizer = CountVectorizer()
X_train = vectorizer.fit_transform(df['pos_tags_str'])
y_train = df['label']

#Initialize KNeighborsClassifier with a chosen value for k (number of neighbors):
knn_classifier = KNeighborsClassifier()

#Train the KNN classifier:
knn_classifier.fit(X_train, y_train)

#Transform testing data using the same vectorizer:
X_test = vectorizer.transform(test_df['pos_tags_str'])
y_test = test_df['label']

#Predict on the testing data:
y_pred = knn_classifier.predict(X_test)

#Evaluate the model:

#Calculate accuracy:
accuracy = accuracy_score(y_test, y_pred)

#Calculate precision:
precision = precision_score(y_test, y_pred)

#Calculate recall:
recall = recall_score(y_test, y_pred)

#Calculate F1 score:
f1 = f1_score(y_test, y_pred)

#Print the evaluation metrics:
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)

#Calculate the predicted probabilities for the positive class:
y_pred_prob = knn_classifier.predict_proba(X_test)[:, 1]

#Calculate the ROC curve:
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)

#Calculate the Area Under the ROC Curve (AUC):
auc = roc_auc_score(y_test, y_pred_prob)

#Print AUC score
print("AUC Score:", auc)

#Plot ROC curve:

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label='ROC Curve (AUC = %0.2f)' % auc)
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

############################################################# Logistic Regression & Bag of words ##################################################################################
from sklearn.linear_model import LogisticRegression

#Initialize CountVectorizer and fit it on training data:
vectorizer = CountVectorizer()
X_train = vectorizer.fit_transform(df['pos_tags_str'])
y_train = df['label']

#Initialize Logistic Regression classifier with increased max_iter:
logreg_classifier = LogisticRegression(max_iter=1000)

#Train the Logistic Regression classifier:
logreg_classifier.fit(X_train, y_train)

#Transform testing data using the same vectorizer:
X_test = vectorizer.transform(test_df['pos_tags_str'])
y_test = test_df['label']

#Predict on the testing data:
y_pred = logreg_classifier.predict(X_test)

#Evaluate the model:

#Calculate accuracy:
accuracy = accuracy_score(y_test, y_pred)

#Calculate precision:
precision = precision_score(y_test, y_pred)

# Calculate recall:
recall = recall_score(y_test, y_pred)

#Calculate F1 score:
f1 = f1_score(y_test, y_pred)

#Print the evaluation metrics:
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)
import numpy as np

# Assuming y_pred contains the predicted values (0s and 1s)
# Count the number of occurrences of each class in y_pred
num_zeros = np.count_nonzero(y_pred == 0)
num_ones = np.count_nonzero(y_pred == 1)

# Print the counts
print("Number of 0 classes predicted:", num_zeros)
print("Number of 1 classes predicted:", num_ones)

#Calculate the predicted probabilities for the positive class:
y_pred_prob = logreg_classifier.predict_proba(X_test)[:, 1]

#Calculate the ROC curve:
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)

#Calculate the Area Under the ROC Curve (AUC):
auc = roc_auc_score(y_test, y_pred_prob)

#Print AUC score
print("AUC Score:", auc)

#Plot ROC curve:

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label='ROC Curve (AUC = %0.2f)' % auc)
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

###################################################################### Naive Bayes & TF-IDF ###################################################################

from sklearn.feature_extraction.text import TfidfVectorizer

#Initialize TfidfVectorizer and fit it on training data:
vectorizer = TfidfVectorizer()
X_train = vectorizer.fit_transform(df['pos_tags_str'])
y_train = df['label']

#Train the Naive Bayes classifier:
nb_classifier = MultinomialNB()
nb_classifier.fit(X_train, y_train)

#Transform testing data using the same vectorizer:
X_test = vectorizer.transform(test_df['pos_tags_str'])
y_test = test_df['label']

#Predict on the testing data:
y_pred = nb_classifier.predict(X_test)

#Evaluate the model:

#Calculate accuracy:
accuracy = accuracy_score(y_test, y_pred)

#Calculate precision:
precision = precision_score(y_test, y_pred)

#Calculate recall:
recall = recall_score(y_test, y_pred)

#Calculate F1 score:
f1 = f1_score(y_test, y_pred)

#Print the evaluation metrics:
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)

#Calculate the predicted probabilities for the positive class:
y_pred_prob = nb_classifier.predict_proba(X_test)[:, 1]

#Calculate the ROC curve:
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)

#Calculate the Area Under the ROC Curve (AUC):
auc = roc_auc_score(y_test, y_pred_prob)

#Print AUC score
print("AUC Score:", auc)

#Plot ROC curve:

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label='ROC Curve (AUC = %0.2f)' % auc)
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

################################################################# Logistic Regression & TF-IDF ###############################################################

#Initialize TfidfVectorizer and fit it on training data:
vectorizer = TfidfVectorizer()
X_train = vectorizer.fit_transform(df['pos_tags_str'])
y_train = df['label']

#Initialize Logistic Regression classifier with increased max_iter:
logreg_classifier = LogisticRegression(max_iter=1000)

#Train the Logistic Regression classifier:
logreg_classifier.fit(X_train, y_train)

#Transform testing data using the same vectorizer:
X_test = vectorizer.transform(test_df['pos_tags_str'])
y_test = test_df['label']

#Predict on the testing data:
y_pred = logreg_classifier.predict(X_test)

#Evaluate the model:

#Calculate accuracy:
accuracy = accuracy_score(y_test, y_pred)

#Calculate precision:
precision = precision_score(y_test, y_pred)

#Calculate recall:
recall = recall_score(y_test, y_pred)

#Calculate F1 score:
f1 = f1_score(y_test, y_pred)

#Print the evaluation metrics:
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)

#Calculate the predicted probabilities for the positive class:
y_pred_prob = logreg_classifier.predict_proba(X_test)[:, 1]

#Calculate the ROC curve:
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)

#Calculate the Area Under the ROC Curve (AUC):
auc = roc_auc_score(y_test, y_pred_prob)

#Print AUC score
print("AUC Score:", auc)

#Plot ROC curve:

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label='ROC Curve (AUC = %0.2f)' % auc)
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

############################################################# Logistic Regression & Word2Vec ##################################################################################

from gensim.models import Word2Vec
import numpy as np

# Train Word2Vec model
word2vec_model = Word2Vec(sentences=df['lemmatized_tokens'], vector_size=100, window=5, min_count=1, sg=1)

#Function to compute document vectors:
def compute_doc_vector(tokens, model):
    doc_vector = np.zeros((model.vector_size,), dtype="float32")
    count = 0
    for word in tokens:
        if word in model.wv:
            doc_vector += model.wv[word]
            count += 1
    if count != 0:
        doc_vector /= count
    return doc_vector

#Compute document vectors for training data:
X_train_word2vec = np.array([compute_doc_vector(tokens, word2vec_model) for tokens in df['lemmatized_tokens']])
y_train = df['label']

#Initialize and train the Logistic Regression classifier:
logreg_classifier = LogisticRegression(max_iter= 1000)
logreg_classifier.fit(X_train_word2vec, y_train)

#Compute document vectors for testing data:
X_test_word2vec = np.array([compute_doc_vector(tokens, word2vec_model) for tokens in test_df['lemmatized_tokens']])
y_test = test_df['label']

#Predict on the testing data:
y_pred = logreg_classifier.predict(X_test_word2vec)

#Evaluate the model:

#Calculate accuracy:
accuracy = accuracy_score(y_test, y_pred)

#Calculate precision:
precision = precision_score(y_test, y_pred)

#Calculate recall:
recall = recall_score(y_test, y_pred)

#Calculate F1 score:
f1 = f1_score(y_test, y_pred)

#Print the evaluation metrics:
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)

#Calculate the predicted probabilities for the positive class:
y_pred_prob = logreg_classifier.predict_proba(X_test_word2vec)[:, 1]

#Calculate the ROC curve:
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)

#Calculate the Area Under the ROC Curve (AUC):
auc = roc_auc_score(y_test, y_pred_prob)

#Print AUC score
print("AUC Score:", auc)

#Plot ROC curve:

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label='ROC Curve (AUC = %0.2f)' % auc)
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

############################################################# KNeighborsClassifier & Word2Vec ####################################################################

#Train Word2Vec model:
word2vec_model = Word2Vec(sentences=df['lemmatized_tokens'], vector_size=100, window=5, min_count=1, sg=1)

#Function to compute document vectors:
def compute_doc_vector(tokens, model):
    doc_vector = np.zeros((model.vector_size,), dtype="float32")
    count = 0
    for word in tokens:
        if word in model.wv:
            doc_vector += model.wv[word]
            count += 1
    if count != 0:
        doc_vector /= count
    return doc_vector

#Compute document vectors for training data:
X_train_word2vec = np.array([compute_doc_vector(tokens, word2vec_model) for tokens in df['lemmatized_tokens']])
y_train = df['label']

#Initialize and train the KNN classifier:
knn_classifier = KNeighborsClassifier()
knn_classifier.fit(X_train_word2vec, y_train)

#Compute document vectors for testing data:
X_test_word2vec = np.array([compute_doc_vector(tokens, word2vec_model) for tokens in test_df['lemmatized_tokens']])
y_test = test_df['label']

#Predict on the testing data:
y_pred = knn_classifier.predict(X_test_word2vec)

#Evaluate the model:

#Calculate accuracy:
accuracy = accuracy_score(y_test, y_pred)

#Calculate precision:
precision = precision_score(y_test, y_pred)

#Calculate recall:
recall = recall_score(y_test, y_pred)

#Calculate F1 score:
f1 = f1_score(y_test, y_pred)

#Print the evaluation metrics:
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)

#Calculate the predicted probabilities for the positive class:
y_pred_prob = knn_classifier.predict_proba(X_test_word2vec)[:, 1]

#Calculate the ROC curve:
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)

#Calculate the Area Under the ROC Curve (AUC):
auc = roc_auc_score(y_test, y_pred_prob)

#Print AUC score
print("AUC Score:", auc)

#Plot ROC curve:

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label='ROC Curve (AUC = %0.2f)' % auc)
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

################################################################### Naive Bayes & Word2Vec #############################################################################

from sklearn.preprocessing import MinMaxScaler

#Initialize StandardScaler:
scaler = MinMaxScaler()

#Train Word2Vec model:
word2vec_model = Word2Vec(sentences=df['lemmatized_tokens'], vector_size=100, window=5, min_count=1, sg=1)

#Function to compute document vectors:
def compute_doc_vector(tokens, model):
    doc_vector = np.zeros((model.vector_size,), dtype="float32")
    count = 0
    for word in tokens:
        if word in model.wv:
            doc_vector += model.wv[word]
            count += 1
    if count != 0:
        doc_vector /= count
    return doc_vector

#Compute document vectors for training data:
X_train_word2vec = np.array([compute_doc_vector(tokens, word2vec_model) for tokens in df['lemmatized_tokens']])

#Fit and transform the training data:
X_train_word2vec_normalized = scaler.fit_transform(X_train_word2vec)
y_train = df['label']

# Initialize and train Naive Bayes classifier:
nb_classifier = MultinomialNB()
nb_classifier.fit(X_train_word2vec_normalized, y_train)

#Compute document vectors for testing data:
X_test_word2vec = np.array([compute_doc_vector(tokens, word2vec_model) for tokens in test_df['lemmatized_tokens']])

#Transform the testing data using the same scaler:
X_test_word2vec_normalized = scaler.transform(X_test_word2vec)
y_test = test_df['label']

#Predict on the testing data:
y_pred = nb_classifier.predict(X_test_word2vec_normalized)

#Evaluate the model:

#Calculate accuracy:
accuracy = accuracy_score(y_test, y_pred)

#Calculate precision:
precision = precision_score(y_test, y_pred)

#Calculate recall:
recall = recall_score(y_test, y_pred)

#Calculate F1 score:
f1 = f1_score(y_test, y_pred)

#Print the evaluation metrics:
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)

#Calculate the predicted probabilities for the positive class:
y_pred_prob = nb_classifier.predict_proba(X_test_word2vec)[:, 1]

#Calculate the ROC curve:
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)

#Calculate the Area Under the ROC Curve (AUC):
auc = roc_auc_score(y_test, y_pred_prob)

#Print AUC score
print("AUC Score:", auc)

#Plot ROC curve:

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label='ROC Curve (AUC = %0.2f)' % auc)
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

############################################################# Logistic Regression & GloVe ##################################################################################

from zipfile import ZipFile
import os

glove_zip_path = '/content/drive/MyDrive/glove.42B.300d.txt'
glove_dir = '/content/drive/MyDrive/'

#Load the GloVe word vectors:
word_embeddings = {}
with open(os.path.join(glove_dir, 'glove.42B.300d.txt'), 'r', encoding='utf-8') as f:
    for line in f:
        values = line.split()
        word = values[0]
        embedding = np.array(values[1:], dtype='float32')
        word_embeddings[word] = embedding

#Define a function to compute document vectors:
def compute_doc_vector(tokens, embeddings):
    doc_vector = np.zeros((300,), dtype="float32")
    count = 0
    for word in tokens:
        if word in embeddings:
            doc_vector += embeddings[word]
            count += 1
    if count != 0:
        doc_vector /= count
    return doc_vector

#Compute document vectors for training data:
X_train_word2vec = np.array([compute_doc_vector(tokens, word_embeddings) for tokens in df['lemmatized_tokens']])
y_train = df['label']

#Initialize and train the Logistic Regression classifier:
logreg_classifier = LogisticRegression(max_iter=1000)
logreg_classifier.fit(X_train_word2vec, y_train)

#Compute document vectors for testing data:
X_test_word2vec = np.array([compute_doc_vector(tokens, word_embeddings) for tokens in test_df['lemmatized_tokens']])
y_test = test_df['label']

#Predict on the testing data:
y_pred = logreg_classifier.predict(X_test_word2vec)

#Evaluate the model:

#Calculate accuracy:
accuracy = accuracy_score(y_test, y_pred)

#Calculate precision:
precision = precision_score(y_test, y_pred)

#Calculate recall:
recall = recall_score(y_test, y_pred)

#Calculate F1 score:
f1 = f1_score(y_test, y_pred)

#Print the evaluation metrics:
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)

#Calculate the predicted probabilities for the positive class:
y_pred_prob = logreg_classifier.predict_proba(X_test_word2vec)[:, 1]

#Calculate the ROC curve:
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)

#Calculate the Area Under the ROC Curve (AUC):
auc = roc_auc_score(y_test, y_pred_prob)

#Print AUC score
print("AUC Score:", auc)

#Plot ROC curve:

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label='ROC Curve (AUC = %0.2f)' % auc)
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

############################################################# KNeighborsClassifier & GloVe ####################################################################

#Define the path to the downloaded GloVe file:
glove_zip_path = '/content/drive/MyDrive/glove.42B.300d.txt'
glove_dir = '/content/drive/MyDrive/'

#Load the GloVe word vectors:
word_embeddings = {}
with open(os.path.join(glove_dir, 'glove.42B.300d.txt'), 'r', encoding='utf-8') as f:
    for line in f:
        values = line.split()
        word = values[0]
        embedding = np.array(values[1:], dtype='float32')
        word_embeddings[word] = embedding

#Define a function to compute document vectors:
def compute_doc_vector(tokens, embeddings):
    doc_vector = np.zeros((300,), dtype="float32")
    count = 0
    for word in tokens:
        if word in embeddings:
            doc_vector += embeddings[word]
            count += 1
    if count != 0:
        doc_vector /= count
    return doc_vector

#Compute document vectors for training data:
X_train_word2vec = np.array([compute_doc_vector(tokens, word_embeddings) for tokens in df['lemmatized_tokens']])
y_train = df['label']

# Initialize and train KNN classifier:
knn_classifier = KNeighborsClassifier()
knn_classifier.fit(X_train_word2vec, y_train)

#Compute document vectors for testing data:
X_test_word2vec = np.array([compute_doc_vector(tokens, word_embeddings) for tokens in test_df['lemmatized_tokens']])
y_test = test_df['label']

#Predict on the testing data:
y_pred = knn_classifier.predict(X_test_word2vec)

#Evaluate the model:

#Calculate accuracy:
accuracy = accuracy_score(y_test, y_pred)

#Calculate precision:
precision = precision_score(y_test, y_pred)

#Calculate recall:
recall = recall_score(y_test, y_pred)

#Calculate F1 score:
f1 = f1_score(y_test, y_pred)

#Print the evaluation metrics:
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)

#Calculate the predicted probabilities for the positive class:
y_pred_prob = knn_classifier.predict_proba(X_test_word2vec)[:, 1]

#Calculate the ROC curve:
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)

#Calculate the Area Under the ROC Curve (AUC):
auc = roc_auc_score(y_test, y_pred_prob)

#Print AUC score
print("AUC Score:", auc)

#Plot ROC curve:

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label='ROC Curve (AUC = %0.2f)' % auc)
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()

##########################################################Naive Bayes & GloVe ###################################################################

#Initialize MinMaxScaler:
scaler = MinMaxScaler()

glove_zip_path = '/content/drive/MyDrive/glove.42B.300d.txt'
glove_dir = '/content/drive/MyDrive/'


#Load the GloVe word vectors:
word_embeddings = {}
with open(os.path.join(glove_dir, 'glove.42B.300d.txt'), 'r', encoding='utf-8') as f:
    for line in f:
        values = line.split()
        word = values[0]
        embedding = np.array(values[1:], dtype='float32')
        word_embeddings[word] = embedding

#Define a function to compute document vectors:
def compute_doc_vector(tokens, embeddings):
    doc_vector = np.zeros((300,), dtype="float32")
    count = 0
    for word in tokens:
        if word in embeddings:
            doc_vector += embeddings[word]
            count += 1
    if count != 0:
        doc_vector /= count
    return doc_vector

#Compute document vectors for training data:
X_train_word2vec = np.array([compute_doc_vector(tokens, word_embeddings) for tokens in df['lemmatized_tokens']])

#Fit and transform the training data:
X_train_word2vec_normalized = scaler.fit_transform(X_train_word2vec)
y_train = df['label']

#Initialize and train the Naive Bayes classifier:
nb_classifier = MultinomialNB()
nb_classifier.fit(X_train_word2vec_normalized, y_train)

#Compute document vectors for testing data:
X_test_word2vec = np.array([compute_doc_vector(tokens, word_embeddings) for tokens in test_df['lemmatized_tokens']])

#Transform the testing data using the same scaler:
X_test_word2vec_normalized = scaler.transform(X_test_word2vec)
y_test = test_df['label']

#Predict on the testing data:
y_pred = nb_classifier.predict(X_test_word2vec_normalized)

#Evaluate the model:

#Calculate accuracy:
accuracy = accuracy_score(y_test, y_pred)

#Calculate precision:
precision = precision_score(y_test, y_pred)

#Calculate recall:
recall = recall_score(y_test, y_pred)

#Calculate F1 score:
f1 = f1_score(y_test, y_pred)

#Print the evaluation metrics:
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)

#Calculate the predicted probabilities for the positive class:
y_pred_prob = nb_classifier.predict_proba(X_test_word2vec)[:, 1]

#Calculate the ROC curve:
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)

#Calculate the Area Under the ROC Curve (AUC):
auc = roc_auc_score(y_test, y_pred_prob)

#Print AUC score
print("AUC Score:", auc)

#Plot ROC curve:

plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label='ROC Curve (AUC = %0.2f)' % auc)
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()
#################################################################################################################################################
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, roc_auc_score
import matplotlib.pyplot as plt

#Define the pipeline:
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer()),
    ('classifier', LogisticRegression(max_iter=1000))
])

#Train the pipeline:
pipeline.fit(df['pos_tags_str'], df['label'])

#Predict on the testing data:
y_pred = pipeline.predict(test_df['pos_tags_str'])

#Evaluate the model:
accuracy = accuracy_score(test_df['label'], y_pred)
precision = precision_score(test_df['label'], y_pred)
recall = recall_score(test_df['label'], y_pred)
f1 = f1_score(test_df['label'], y_pred)

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)

#Calculate the predicted probabilities for the positive class:
y_pred_prob = pipeline.predict_proba(test_df['pos_tags_str'])[:, 1]

#Calculate the ROC curve:
fpr, tpr, thresholds = roc_curve(test_df['label'], y_pred_prob)

#Calculate the Area Under the ROC Curve (AUC):
auc = roc_auc_score(test_df['label'], y_pred_prob)
print("AUC Score:", auc)

#Plot ROC curve:
plt.figure(figsize=(8, 6))
plt.plot(fpr, tpr, color='blue', lw=2, label='ROC Curve (AUC = %0.2f)' % auc)
plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()